tests - alerts in job json should match alerts.json

ledger tests should mostly pass
livestock should pass
depreciation should pass








excel plugin - lodgment -> lodgement




<account name="Investment Gains/-Losses" parent_role="Gains" role="Trading_Accounts!Investment Gains/-Losses"/>
&^ action_verbs sheet needs to be adjusted.
 - test





1) full video of usage

	div7a video -
	    compare with ato calc
    	show the input sheet
	    walk through prolog code

    
	videos for how to play with whatever we're doing

      for how to modify the taxonomy (example)
		^
	      present IC example 1(again?)






2) endpoint tests automated again






Financial_Investments vs FinancialInvestments...?






how to speed it up:
perf for 1000x size?     
     scryer?
     datalog(E) - new engine

  procedural rewrite (py, c++)






crypto

      how how to generate data from a data export generated by a big crypto exchange or wallet

      crypto historical conversion rates
        - find an api, similar to openexchangerates.org
          - openexchangerates.org actually does provide BTC and some other crypto rates
          - try https://exchangerate.host/ next?






start using an actual database for users and data? But this would not bring true isolation without isolating untrusted code (swipl). Figure out a cloud provider where robust can spawn ephemeral worker VMs. Or docker containers?
at any case:
	 the frontend would enqueue jobs as usual, but,
		then it would ask the cloud provider or its own hypervisor to spawn a 
			stack of 1) worker 2) pyservices 3) c#services
...







sdk2 - test:

1) these should be equivalent:
	tests2/endpoint_tests/ledger/Robust Input Example 8.7.2021
	RdfTemplates.n3: IC Example 1
	~/'IC example 1.n3'

finish - 
2) with output of python sdk - with l:date properties on exchange rate items
	
	
	
	
	
---------








/*
wip: go through all GL sheets in excel
	automatize or
	set non-default phase (smsf_...)

*/












#ifdef DEV for localhost servers




livestock standalone needs frontend server support for requesting xml result



failing because of a bug in calling arelle:
        ['endpoint_tests/ledger/ledger--with-schemaref']): testcase_error(http_code_400)
        ['endpoint_tests/ledger/historical_vs_current1']): testcase_error(http_code_400)
move arelle into python_server.
   


	reset_gensym(iri), % because we use gensym in investment reports and it will keep incrementing
	throughout the test-cases, causing fresh responses to not match saved responses.


/*
todo:
debug
    This option enables or disables the possibility to debug the CHR code. Possible values are on (default) and off. See section 9.4 for more details on debugging. The default is derived from the Prolog flag generate_debug_info, which is true by default. See -nodebug. If debugging is enabled, optimization must be disabled.
generate_debug_info(bool, changeable)
    If true (default) generate code that can be debugged using trace/0, spy/1, etc. Can be set to false using the -nodebug. This flag is scoped within a source file. Many of the libraries have :- set_prolog_flag(generate_debug_info, false) to hide their details from a normal trace.20
swipl --nodebug
anyway, :- set_prolog_flag(generate_debug_info, false)
    --pldoc[=port]           Start PlDoc server [at port]
    --home=DIR               Use DIR as SWI-Prolog home
    --dump-runtime-variables[=format]
                        Dump link info in sh(1) format
editor(atom, changeable)
    Determines the editor used by edit/1. See section 4.4.1 for details on selecting the editor used.
backtrace_goal_depth(integer, changeable)
    The frame of a backtrace is printed after making a shallow copy of the goal. This flag determines the depth to which the goal term is copied. Default is `3'.
xref(bool, changeable)
    If true, source code is being read for analysis purposes such as cross-referencing. Otherwise (default) it is being read to be compiled. This flag is used at several places by term_expansion/2 and goal_expansion/2 hooks, notably if these hooks use side effects. See also the libraries library(prolog_source) and library(prolog_xref).
    */





testcases - client_version is "test_runner" - endpoint will allow that.










split out internal_workers modules used by frontend_server
secrets
ports - internally, bind to some new ports, like 6666, to make it less confusing
docker confiigurations - django dev vs prod
RabbitMQ contains functionality which explicitly tracks and manages memory usage, and thus needs to be made aware of cgroup-imposed limits.

# security issues:
# internal_services is a HTTP django RPC server that handles some OS interaction and other tasks like XML validation for prolog. This includes executing arbitrary shell commands.  If an attacker can make our code invoke http requests to arbitrary URLs, they get shell access. There is no authentication. And against standards, it invokes actions on GET requests, but that is just a convenience and can be disabled for production, and disabling that is probably all we need to do.
# invoking http GET requests to arbitrary URLs is possible through:
#  arelle, by referencing them in xbrl files referenced by users
#  get_file_from_url_into_dir, used for fetching additional input files specified by user


# calls to load_xml from prolog: opening arbitrary files


# todo: docker secrets, docker configs, try to keep sensitive info in files..














Net Income (Loss) [Roll Up]
	Income (Loss) from Continuing Operations Before Tax [Roll Up]
	Income Tax Expense (Benefit)
	


- 	push_context('processing phases (each phase depends on posted results of previous phase):'),















https://stackoverflow.com/questions/21711519/how-can-i-periodically-call-statistics-2-in-swi-prolog












# js o365 frontend? (postponed)

rewrite excel plugin into js
The core logic will be the same as the C# version, but:

* different RDF library may require somewhat different management of loaded kb, for example

* sheet/workspace (create new sheet, list sheets..) api will be different: adding a bit of abstraction there will make sense, to document expectations/requirements of the glue code / of the actual api.

* sheet limits / end of sheet / end of list detection

cell data reading/writing, data types: 
* already somewhat messy in c# and not taking advantage of all excel features. 
* how/what types are declared in templates, and how this translates to what excel api functions are used to reading / writing corresponding cells.
* different users may have different style, some may set a sheet to monetary format, some leave it as text. 









http://lorenae.github.io/qb4olap/










swarm + https://forums.docker.com/t/docker-swarm-mode-and-local-images/22894/10

